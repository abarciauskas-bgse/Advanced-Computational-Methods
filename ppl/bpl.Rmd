---
title: "Bayesian Program Learning: MNIST Data Set"
author: "Aimee Barciauskas & Miquel Torrens"
date: "March 2, 2016"
output: pdf_document
---

Bayesian Programming Learning of Hand-Drawn Integers

[ADD ME: SHOW EXAMPLES]
0
1
2
3
4
5
6
7
8
9

1. Create thinned image
2. Generate directed path

Directed path enables:

1. New integer generation
2. Prediction?

## Creating the thinned image

```{r}
if (!require('grid')) install.packages('grid')
if (!require('prodlim')) install.packages('prodlim')
if (!require('igraph')) install.packages('igraph')
if (!require('RColorBrewer')) install.packages('RColorBrewer')
if (!require('scales')) install.packages('scales')
if (!require('MASS')) install.packages('MASS')

setwd('~/Box Sync/abarciausksas/myfiles/Advanced Computational Methods/data/')
digits <- read.csv('MNIST_training.csv')

source('~/Box Sync/abarciausksas/myfiles/15D012 Advanced Computational Methods/datasets/MNIST/displayDigit.R')
source('~/Box Sync/abarciausksas/myfiles/Advanced Computational Methods/ppl/utils.R')


source('~/Box Sync/abarciausksas/myfiles/Advanced Computational Methods/ppl/thinPoints.R')
source('~/Box Sync/abarciausksas/myfiles/Advanced Computational Methods/ppl/allThinnedInts.R')
# for 1000 points this took 3 minutes? that seems wrong
rand.idcs <- sample(1:nrow(digits),100)
system.time(thinned.ints <- all.thinned.ints(digits[rand.idcs,]))
```

Make sure things worked:

```{r}
for (i in 1:length(rand.idcs)) {
  plot(thinned.ints[[i]]$points, main = paste0('Hey, this is a: ', thinned.ints[[i]]$label), pch = 19, ylim = c(-16,0), xlim = c(0,16))
  Sys.sleep(0.5)
}
```


```{r}
# Collect a training set
rand.idcs <- sample(1:length(thinned.ints),100)
# Collect a train set
train.set <- list()
train.labels <- c()
while (length(train.labels) < 10) {
  for (i in 1:length(rand.idcs)) {
    i <- rand.idcs[i]
    label <- thinned.ints[[i]]$label
    if (!(label %in% train.labels)) {
      train.labels <- append(train.labels, label)
      train.set <- append(train.set, list(thinned.ints[[i]]))
    }
  }
}

for (i in 1:length(train.set)) {
  plot(train.set[[i]]$points, main = paste0('Hey, this is a: ', train.set[[i]]$label), pch = 19, ylim = c(-16,0), xlim = c(0,16))
  Sys.sleep(0.5)
}
```

And a test set:

```{r}
# FIXME: NEED TO SET DIFF FROM TRAINING SET
rand.idcs <- sample(1:length(thinned.ints),90)
# Collect a test set
test.set <- list()
test.labels <- c()
for (i in 1:length(rand.idcs)) {
  i <- rand.idcs[i]
  label <- thinned.ints[[i]]$label
  test.labels <- append(test.labels, label)
  test.set <- append(test.set, list(thinned.ints[[i]]))
}

for (i in 1:10) {
  plot(test.set[[i]]$points, main = paste0('Hey, this is a: ', test.set[[i]]$label), pch = 19, ylim = c(-16,0), xlim = c(0,16))
  Sys.sleep(0.5)
}
```

Can we generate paths to predict 0's and 1's

```{r}
source('../ppl/trainPath.R')
train.objects <- list()
for (i in 0:9) {
  digit <- train.set[[which(sapply(train.set, function(dig) { dig$label == i }))]]
  train.objects[[i+1]] <- train.digit(digit)
}

# test an object
errors <- 0
for (i in 1:length(test.set)) {
  test.digit <- sample(test.set,1)[[1]]
  estimate <- train.digit(test.digit)
  estimate$step.probs
  res <- lapply(train.objects, function(train.obj) {
    step.prob <- estimate$step.probs%*%train.obj$step.probs
    # weight by the difference in length, this should be scaled somehow
    length.prob <- 1/abs((train.obj$total.steps - estimate$total.steps))
    # weight by the difference in strokes, this may be too much of a weight
    strokes.prob <- 1/6*abs((train.obj$num.real.strokes - estimate$num.real.strokes))
    return(step.prob*length.prob*strokes.prob)
  })
  pred <- which.max(res)-1
  actual <- test.digit$label
  if (pred != actual) errors <- errors + 1
}
(accuracy <- errors/length(test.set))
```

Test this on test data

```{r}
zero.test <- test.set[[which(sapply(test.set, function(dig) { dig$label == 0 }))]]
one.test <- test.set[[which(sapply(test.set, function(dig) { dig$label == 1 }))]]
# create the path
zero.test <- generatePaths(list(zero.test))
one.test <- generatePaths(list(one.test))

# create step distributions
(zero.total.steps <- length(zero.test[[1]]$direction.steps[[1]]))
(zero.test.step.probabilities <- rep(0, 8))
for (step.type in 1:8) {
  # find number of times step happened in the first stroke
  zero.test.step.probabilities[step.type] <- sum(zero.test[[1]]$direction.steps[[1]] == step.type)/zero.total.steps
}
fractions(zero.test.step.probabilities)

one.steps <- one.test[[1]]$direction.steps[[which.max(sapply(one.test[[1]]$direction.steps, length))]]
(one.total.steps <- length(one.steps))
(one.test.step.probabilities <- rep(0, 8))
for (step.type in 1:8) {
  # find number of times step happened in the first stroke
  one.test.step.probabilities[step.type] <- sum(one.test[[1]]$direction.steps[[1]] == step.type)/one.total.steps
}
fractions(one.test.step.probabilities)

one.test.step.probabilities%*%zero.train.step.probabilities
one.test.step.probabilities%*%one.train.step.probabilities

zero.test.step.probabilities%*%one.train.step.probabilities
zero.test.step.probabilities%*%zero.train.step.probabilities
```



Can we compare lengths?

```{r}
colors <- brewer.pal(11,'Spectral')

int.lengths <- lapply(thinned.ints, function(num) {
  c(num$label, num$num.pixels)
})

(int.lengths.mat <- matrix(unlist(int.lengths), nrow = length(int.lengths), ncol = 2, byrow = TRUE))
int.lengths.mat <- data.frame(na.omit(int.lengths.mat))
colnames(int.lengths.mat) <- c('label', 'num.pixels')

plot(int.lengths.mat$num.pixels ~ int.lengths.mat$label, pch = 19, col = alpha(colors[[9]], 0.1))

boxplot(int.lengths.mat$num.pixels ~ int.lengths.mat$label, pch = 19, col = alpha(colors[[9]], 0.5))
```

Looks like we can predict based on length a bit.

Generating strokes

```{r}
source('../ppl/relativePosition.R')
source('../ppl/addPaths.R')

dev.off()
#rand.idcs <- sample(1:length(thinned.ints),1000)
# FIXME: preforms poorly if too many points - maybe rethin?
thinned.ints.with.paths <- generatePaths(thinned.ints[1:100])

for (i in 1:length(thinned.ints.with.paths)) {
  plot(thinned.ints.with.paths[[i]]$points, pch = 19, ylim = c(-16,0), xlim = c(0,16))
  points(thinned.ints.with.paths[[i]]$path, pch = 19, col = 'red')
  Sys.sleep(1)
}
```

Features of stroke paths

```{r}

int.loops <- lapply(thinned.ints.with.paths, function(num) {
  c(num$label, num$loops)
})

int.loops.mat <- matrix(unlist(int.loops), nrow = length(int.loops), ncol = 2, byrow = TRUE)
colnames(int.loops.mat) <- c('label','loops')
int.loops.d <- data.frame(int.loops.mat)

plot(int.loops.d$loops ~ int.loops.d$label, pch = 19, col = alpha(colors[[9]], 0.1))

boxplot(int.loops.d$loops ~ int.loops.d$label, pch = 19, col = alpha(colors[[9]], 0.1))
```

```{r}
# long command to print out a bunch of stroke lengths, to determine what a good cutoff is, so decided by heuristics, like most of this file and functions it uses :)
# sapply(thinned.ints.with.paths, function(p) { print(sapply(p$direction.steps, length))})
int.strokes <- lapply(thinned.ints.with.paths, function(num) {
  num.real.strokes <- sum(sapply(num$direction.steps, length) >= 8)
  c(num$label, num.real.strokes)
})

int.strokes.mat <- matrix(unlist(int.strokes), nrow = length(int.strokes), ncol = 2, byrow = TRUE)

colnames(int.strokes.mat) <- c('label','num.real.strokes')
int.strokes.d <- data.frame(int.strokes.mat)

plot(int.strokes.d$num.real.strokes ~ int.strokes.d$label, pch = 19, col = alpha(colors[[9]], 0.1))

boxplot(int.strokes.d$num.real.strokes ~ int.strokes.d$label, pch = 19, col = alpha(colors[[9]], 0.1))
```

Can we predict based on stroke and loop data?
```{r}
pts <- thinned.ints.with.paths
mat <- matrix(NA, nrow = length(pts), ncol = 4)
colnames(mat) <- c('label','num.pixels','strokes','loops')

for (i in 1:nrow(mat)) {
  values <- sapply(colnames(mat), function(n) {as.numeric(pts[[i]][n])})
  mat[i,] <- values
}

library(nnet)
df <- as.data.frame(mat)
df$label <- factor(df$label)
mod <- multinom(formula = label ~ ., df)
head(mod$fitted.values)
preds <- apply(mod$fitted.values, 1, which.max)
#bad

library(class)
nrow(df)
feats <- c('loops','strokes','num.pixels')
train <- df[1:900,feats]
test <- df[901:1000,feats]
real <- df[901:1000,'label']
cl <- df[1:900,'label']
k.mod <- knn(train = train, test = test, cl = cl)
head(k.mod)
head(real)
```

Generating new digits

```{r}
for (idx in 1:length(pts)) {
  d <- pts[[idx]][['path']]
  # in a 16x16 image, a point is positioned at the absolute value of y - 1 + x
  d.points <- apply(d, 1, function(pt) { (16*abs(pt[2])-1)+pt[1] })
  d.points.dir <- rep(d.points,each=2)[2:(2*length(d.points)-1)]
  g.dir <- make_directed_graph(d.points.dir)
  paths <- all_simple_paths(g.dir, 36)
  
  mat <- matrix(0, nrow = 16, ncol = 16)
  for (i in 1:256) {
    if (i %in% d.points) {
      row.idx <- i %/% 16 + 1
      col.idx <- i %% 16
      mat[row.idx,col.idx] <- 1
      # randomly add noise to surrounding pixels
      # randomly add pixel
    }
  }

  features <- as.numeric(t(mat))
  # displaying the original digit
  png('dig.png',width = 32, height = 32)
  displayDigit(features, pts[[idx]]$label, newDevice = FALSE)
  dev.off()
  library(png)
  #install.packages('mmand')
  library(mmand)
  library(RNiftyReg)
  image <- readPNG('dig.png')
  display(image)
  affine <- buildAffine(skews=0.01, source=image)
  print(affine)
  img_skewed <- applyTransform(affine, image)
  display(img_skewed[,,1])
  Sys.sleep(1)
}
```