{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression analysis in distributed computing \n",
    "\n",
    "\n",
    "First thing to note is that with big amounts of data closed form solution through normal equations is not feasible anymore:\n",
    "\n",
    "$$\\mathbf{w} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Computing $X^T X$ term takes $O(nk^2)$ operations, while inverse takes $O(k^3)$, where $n$ is number of observations and $k$ is number of variables. Storage is also considerable, $O(k^2)$ for these matrices and $O(nk)$ for matrix $X$.\n",
    "\n",
    "How can we deal with these issues? Depends on the type of the problem we have.  \n",
    "\n",
    "1. **Few variables, but many observations**. Storing $X$ can be solved by distributed form of storage, while computing $X^T X$ matrix can be done through a sum of outer products instead of computing inner products. Outer products work great as we can do them separately on each node and then sum them together in the main node. This is feasible if number of dimensions is not that big, in such case matrices of size $k^2$ should be able to fit on a single node.  \n",
    "2. **Many variables, and many observations**. Outer product operation is now also not feasible as matrices of size $k^2$ do not fit in any single node. In these situations we can potentially exploit sparsity to reduce the storage and computational requirements. We can also try to reduce the dimensionality through unsupervised learning techniques, such as SVD or PCA. However, the way out is really to discard the closed form approach and use numerical optimization - (stochastic) gradient descent needs less operations, $O(nk)$, and uses far less storage, $O(k)$.\n",
    "\n",
    "\n",
    "# Logistic regression\n",
    "\n",
    "Now we will examine how we can use a chain map-reduce operations to train the logistic regression with gradient descent. We will use the Bank marketing dataset from the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) to illustrate these operations. It is not a big dataset but it will serve to illustrate the principles. We could have worked with bigger datasets, but it would have involved a lot of waiting for some of the steps.\n",
    "\n",
    "\n",
    "Outline: \n",
    "\n",
    "1. Create an RDD out of the text file \n",
    "2. Parse text lines into variables  \n",
    "3. Normalize variables\n",
    "4. Create training, validation and test set\n",
    "5. Baseline model \n",
    "6. Logistic regression through gradient descent  \n",
    "7. Logistic regression through the MLlib Spark library\n",
    "\n",
    "\n",
    "## 1. Creating an RDD and first look at the data\n",
    "\n",
    "I provided you with the dataset in a csv format, use that one instead of original dataset from the repository. I only removed the categorical variables. These need special processing operations that we will not cover. You should load the file directly from your bucket and convert it to the RDD using the appropriate function. You should have created a bucket already on S3 and uploaded the dataset there. \n",
    "\n",
    "Following the instructions in the comments, fill in the parts in the code below instead of (YOUR_CODE) parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_ACCESS_KEY_ID=AKIAIPGASHRV3Q4QAS5Q\n",
      "env: AWS_SECRET_ACCESS_KEY=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\n",
      "download: s3://abarciauskas-bgse/data/bank.csv to ./bank.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of workers you have started\n",
    "numPartitions = 4\n",
    "%env AWS_ACCESS_KEY_ID=AKIAIPGASHRV3Q4QAS5Q\n",
    "%env AWS_SECRET_ACCESS_KEY=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\n",
    "\n",
    "!aws s3 cp s3://abarciauskas-bgse/data/bank.csv bank.csv\n",
    "    \n",
    "# we are loading the text file directly from our bucket and converting it to RDD\n",
    "# use appropriate function and set the number of partitions\n",
    "rdd = sc.textFile(\"s3://abarciauskas-bgse/data/bank.csv\", numPartitions)\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41188\n",
      "[u'0,56,261,4.857,5191,0,1.1,93.994,-36.4', u'0,57,149,4.857,5191,0,1.1,93.994,-36.4', u'0,37,226,4.857,5191,0,1.1,93.994,-36.4']\n"
     ]
    }
   ],
   "source": [
    "# use appropriate function to count the number of observations in the dataset\n",
    "noObs = rdd.count()\n",
    "print noObs\n",
    "\n",
    "# use appropriate function to get only 3 observations from the whole dataset\n",
    "noObservations = 3\n",
    "fewObs = rdd.take(noObservations)\n",
    "print fewObs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# checking if you got a correct result, you should have 11 million observations\n",
    "# you will see an output here only if you made an error\n",
    "assert noObs == 41188, \"Something is wrong here!\"\n",
    "assert len(fewObs) == noObservations, \"You did not extract correct number of observations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Parsing the text lines\n",
    "\n",
    "From the print of `fewObs` object you should realize that each observation in the current RDD is one long line of text, and not nice a data frame. \n",
    "\n",
    "One line should look something like this: \n",
    "\n",
    "`u'1,8.692932128906250000e-01,-6.350818276405334473e-01,2.256902605295181274e-01,...'`. \n",
    "\n",
    "First point is a label, either a 0 or 1, and remaining numbers are features.\n",
    "\n",
    "Your first task is to parse the text lines and create variables out of them. We will convert them to a special class `LabeledPoint` from MLlib library (see official docs [here](http://spark.apache.org/docs/latest/mllib-data-types.html)). This is the format used for storing supervised learning data in MLlib, both classification and regression. \n",
    "\n",
    "You will fill out the details of the `parseTextLine` function that processes one line of text and returns a LabeledPoint object. You will check it out on couple of observations that you extracted above, on `fewObs` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.2\n"
     ]
    }
   ],
   "source": [
    "# numpy version should be at least 1.4 for MLlib, there is a ridiculous bug that raises an exception\n",
    "# because recent numpy is named 1.10 which is mathematically not higher than 1.4\n",
    "# the bootstrap script installs previous version, 1.9.2 for this reason\n",
    "import numpy as np\n",
    "print np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[57.0,149.0,4.857,5191.0,0.0,1.1,93.994,-36.4]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "# you should fill out details of function parseTextLine\n",
    "# input: line, a single observation consisting of a string\n",
    "# output: LabeledPoint instance, text converted to distinct variables, label and features\n",
    "# use Python's split() method to separate elements of the text lines\n",
    "def parseTextLine(line):\n",
    "    attrs = line.split(\",\")\n",
    "    label = attrs[0]\n",
    "    features = attrs[1:len(attrs)]\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "# check if it works on fewObs object by applying parseTextLine(), the result should be a list\n",
    "parsedLines = [parseTextLine(line) for line in fewObs]\n",
    "print type(parsedLines)\n",
    "\n",
    "# now, print out the features and label separately for the first observation, \n",
    "# using the LabeledPoint.features and LabeledPoint.label attributes\n",
    "print parsedLines[1].features\n",
    "print parsedLines[1].label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(parsedLines[1].features) == 8, 'You should have 8 features!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If everything seems to be in order, proceed by parsing all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use your parseTextLine function on every observation\n",
    "parsedRDD = rdd.map(parseTextLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Normalizing the features \n",
    "\n",
    "Standardizing or normalizing features is very relevant for algorithms using distance measures. In our situation it will also bring significant improvements since we will use (stochastic) gradient descent to train the logistic regression. We will use a simple transformation to 0-1 interval\n",
    "\n",
    "$$ x_{norm} = (x-min(x)) / (max(x)-min(x)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# take a single row and compute the number of features in it\n",
    "noFeatures = len(parsedRDD.take(1)[0].features)\n",
    "print noFeatures  # you should get 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "[17.0, 0.0, 0.63400000000000001, 4963.6000000000004, 0.0, -3.3999999999999999, 92.200999999999993, -50.799999999999997]\n",
      "[98.0, 4918.0, 5.0449999999999999, 5228.1000000000004, 7.0, 1.3999999999999999, 94.766999999999996, -26.899999999999999]\n"
     ]
    }
   ],
   "source": [
    "# get min and max for each feature \n",
    "featuresMin = []; featuresMax = []\n",
    "for i in range(noFeatures):\n",
    "    print i\n",
    "    feature = parsedRDD.map(lambda lp: lp.features[i])\n",
    "    featuresMin.append(feature.min())\n",
    "    featuresMax.append(feature.max())\n",
    "\n",
    "# print out minimum and maximum value of each feature\n",
    "print featuresMin\n",
    "print featuresMax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# define a function for normalizing each feature according to min and max information\n",
    "# input: lp, LabeledPoint, a single observation\n",
    "#        featuresMin, a list of minimum values for each fature\n",
    "#        featureMax, a list of maximum values for each feature\n",
    "# output: a LabeledPoint with original label, but new, transformed features\n",
    "def normalize(lp, featuresMin, featuresMax):\n",
    "    normed_features = list()\n",
    "    for i in range(0,len(lp.features)):\n",
    "        normed_features.insert(i, (lp.features[i] - featuresMin[i])/(featuresMax[i] - featuresMin[i]))\n",
    "    lp = LabeledPoint(lp.label, normed_features)\n",
    "    return lp\n",
    "\n",
    "# run normalize() on all observations\n",
    "normedRDD = parsedRDD.map(lambda x: normalize(x, featuresMin, featuresMax))\n",
    "\n",
    "# if everything went all right, you should see 0 and 1 as a result here\n",
    "print normedRDD.map(lambda lp: lp.features[0]).min()\n",
    "print normedRDD.map(lambda lp: lp.features[0]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Creating training, validation and test data\n",
    "\n",
    "Now that we have created proper dataset we will create separate training, validation and test data. You should use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) to randomly create three separate datasets, use the weights and seed supplied below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not change the weights and the seed\n",
    "weights = [.7, .15, .15]\n",
    "seed = 1111\n",
    "\n",
    "# use randomSplit with weights and seed defined above on the trainRDD\n",
    "trainRDD, valRDD, testRDD = normedRDD.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should [cache](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache) all three of new RDD's, as you will be using them a lot in the remainder of the problem set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 28940.000, Validation: 6121.000, Test: 6127.000, All: 41188.000\n"
     ]
    }
   ],
   "source": [
    "# cache the data\n",
    "trainRDD.cache()\n",
    "valRDD.cache()\n",
    "testRDD.cache()\n",
    "\n",
    "# get the number of observation in each subset\n",
    "noTrain = trainRDD.count()\n",
    "noVal = valRDD.count()\n",
    "noTest = testRDD.count()\n",
    "\n",
    "print \"Training: {0:.3f}, Validation: {1:.3f}, Test: {2:.3f}, All: {3:.3f}\".format(noTrain, noVal, noTest, noTrain + noVal + noTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "assert noTrain == 28940, \"Wrong number of observations. Did you change the weights?\"\n",
    "assert noVal == 6121, \"Wrong number of observations. Did you change the weights?\"\n",
    "assert noTest == 6127, \"Wrong number of observations. Did you change the weights?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Baseline model\n",
    "\n",
    "For every analysis you should have a good benchmark or baseline as evaluating performance of models is difficult without a reference frame. How do you know whether an accuracy of 75% is a good performance or a bad performance?  \n",
    "\n",
    "A simplest reference frame is how well could you do with no learning what so ever, that is, without extracting any information from features. In classification problem that we have here, this is a simple proportion of labels in the dataset. In prediction terms, based on this model we will always make the same prediction independent of the features - our average label is the constant prediction value.  If our model is doing worse than this, we are doing something very wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.112370421562\n"
     ]
    }
   ],
   "source": [
    "# compute mean over label part of LabeledPoints\n",
    "meanLabel = trainRDD.map(lambda x: x.label).mean()\n",
    "\n",
    "print meanLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.allclose(meanLabel, 0.112370421562), \"Something is wrong, check your calculation of the mean label\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have obtained lower proportion of successful calls than 0.5, hence the prediction of the baseline model for each observation is 0. \n",
    "\n",
    "How does the baseline model performs? We will use a misclassification error as a measure of performance. You will now write a function that evaluates the accuracy of each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333333333333\n"
     ]
    }
   ],
   "source": [
    "# classify function should take two arguments\n",
    "# inputs: probability, float, that the observation is 1\n",
    "#         threshold, float, for classifying predicted probability as 0 or 1\n",
    "# output: float, predicted label, either 0 or 1\n",
    "def classify(probability, threshold):\n",
    "    # you can ignore the ties\n",
    "    if probability > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# this function should compute the misclassification error for a single observation and its prediction,\n",
    "# you should use classify function to convert probability into predicted labels\n",
    "# input: probability, float between 0 and 1\n",
    "#       AIMEE: IS THIS PREDICTED LABEL OR TRUE LABEL? Makes more sense as true label\n",
    "#        predictedLabel, float, either 0 or 1, \n",
    "#        threshold, float, for classifying predicted probability as 0 or 1\n",
    "# output: float, 0 if correct and 1 if incorrect\n",
    "def misErrorSingle(probability, label, threshold):\n",
    "    if classify(probability, threshold) != label:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "\n",
    "# this function should compute mean misclassification error on RDD labPred (see example below)\n",
    "# it should apply the function misErrorSingle on the whole RDD\n",
    "# input: labPredRDD, a label prediction tuples\n",
    "#        threshold, float, for classifying predicted probability as 0 or 1\n",
    "# output: mean misclassification error\n",
    "def misError(labPredRDD, threshold):\n",
    "    errors = (labPredRDD\n",
    "              .map(lambda x: misErrorSingle(x[0], x[1], threshold))\n",
    "              .collect())\n",
    "    return sum(errors)/float(labPredRDD.count())\n",
    "\n",
    "# check it on an easy example, 1 observation misclassified, 2 correct\n",
    "labPredRDD_ex = sc.parallelize([(0., 1.), (0., 0.), (1., 1.)])\n",
    "threshold = 0.5\n",
    "misError_ex = misError(labPredRDD_ex, threshold)\n",
    "print misError_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(misError_ex, 0.333333333333), 'incorrect value for misError_ex'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now make predictions based on the baseline model for all three datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model - Train misclassification error = 0.112\n",
      "Baseline model - Validation misclassification error = 0.110\n",
      "Baseline model - Test misclassification error = 0.117\n"
     ]
    }
   ],
   "source": [
    "# set the threshold for evaluating the probabiliteis\n",
    "threshold = 0.5\n",
    "\n",
    "# for each dataset first create RDD's of tuples with meanLabel and label in each tuple\n",
    "trainMean = round(trainRDD.map(lambda x: x.label).mean(), 0)\n",
    "# and then feed these RDD's into misError function\n",
    "baseTrain = trainRDD.map(lambda x: (x.label, trainMean))\n",
    "baseTrain_misError = misError(baseTrain)\n",
    "\n",
    "valMean = round(valRDD.map(lambda x: x.label).mean(), 0)\n",
    "# and then feed these RDD's into misError function\n",
    "baseVal = valRDD.map(lambda x: (x.label, valMean))\n",
    "baseVal_misError = misError(baseVal)\n",
    "\n",
    "testMean = round(testRDD.map(lambda x: x.label).mean(), 0)\n",
    "# and then feed these RDD's into misError function\n",
    "baseTest = testRDD.map(lambda x: (x.label, testMean))\n",
    "baseTest_misError = misError(baseTest)\n",
    "\n",
    "print 'Baseline model - Train misclassification error = {0:.3f}'.format(baseTrain_misError)\n",
    "print 'Baseline model - Validation misclassification error = {0:.3f}'.format(baseVal_misError)\n",
    "print 'Baseline model - Test misclassification error = {0:.3f}'.format(baseTest_misError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Training logistic regression with Gradient descent\n",
    "\n",
    "Now let's see how much better we can do with a simple logistic regression. We will train it with the gradient descent method since analytical solutions require too much storage and computing time. \n",
    "\n",
    "Recall from the first term the equation for the gradient descent update for logistic regression: \n",
    "\n",
    "$$ \\mathbf{w}_{i+1} = \\mathbf{w}_i - \\alpha \\sum_n (\\mathbf{w}_i^\\top\\mathbf{x}_n  - y_n) \\mathbf{x}_n \\,.$$ \n",
    "\n",
    "where $i$ is the iteration number, and $n$ denotes observation. We will use constant learning rate, but there are various schemes for decreasing learning rates, such as $\\alpha=\\frac{constant}{n\\sqrt{i}}$.\n",
    "\n",
    "Although this is computationally and storage wise much cheaper than analytical solution, there are disadvantages as well. Gradient part is computed in each node and sent to the driver (map step), after sum of gradients is computed in the driver (reduce step), updated weights $\\mathbf{w}_{i+1}$ have to be redistributed over the network to all the nodes for the next iteration. Hence, there will be a lot of communication over network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You will first define a log loss function. Misclassification error that we defined above is great for getting a sense of absolute performance of the algorithm, however it is too rough to use it for monitoring the gradient descent process. You will first define function `logLossSingle` that computes the loss for a single observation, and then function `logLoss` that will implement it in parallel on whole RDD.\n",
    "\n",
    "Given independent observations likelihood function for binary classification problems can be defined as\n",
    "\n",
    "$$ L(\\theta) = \\prod_{i=1}^N p(y_i|x_i\\theta) = \\prod_{i=1}^N (p_i)^{y_i} (1-p_i))^{1-y_i}$$\n",
    "\n",
    "Since we would be computing very small numbers we would quickly run into problems with numerical precisions. Hence we actually compute a log of this expression, which is a monotonic transformation and no information is lost\n",
    "\n",
    "$$ logL(\\theta) = - \\sum_{i=1}^N y_i log(p_i) + (1-y_i)(1-p_i) $$\n",
    "\n",
    "To convert it into a minimization problem we multiply it additionally with -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69314718056\n",
      "0.0100503358535\n",
      "4.60517018599\n",
      "25.3284360229\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "# input: prob, float, value between 0 and 1, predicted probability that label=1\n",
    "#        lab, float, label of the observation, either 0 or 1\n",
    "# output: float, negative log likelihood of a single observation\n",
    "def logLossSingle(prob, lab):\n",
    "    # if probability is too small/large add/subtract this epsilon value\n",
    "    # this is because log(0) is not defined\n",
    "    epsilon = 10e-12\n",
    "    if prob < epsilon:\n",
    "        prob = epsilon\n",
    "    return -(lab*log(prob) + (1-lab)*log(1-prob))\n",
    "\n",
    "# if you get an error on any of these check your computations, especially epsilon\n",
    "print logLossSingle(.5, 1)   # 0.69314718056\n",
    "print logLossSingle(.99, 1)  # 0.0100503358535\n",
    "print logLossSingle(.01, 1)  # 4.60517018599\n",
    "print logLossSingle(0, 1)    # 25.3284360229\n",
    "#print logLossSingle(1, 1)    # 1.00000008275e-11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.30116251751\n"
     ]
    }
   ],
   "source": [
    "# logLoss function should compute the mean log loss on the whole RDD probLab \n",
    "# input: probLab, RDD of probability label tuples\n",
    "# output: float, mean log loss\n",
    "def logLoss(probLab):\n",
    "    # apply the function logLossSingle on the whole RDD and compute the mean\n",
    "    ll = probLab.map(lambda x: logLossSingle(x[0], x[1])).mean()                     \n",
    "    return ll\n",
    "\n",
    "# check it on an easy example\n",
    "probLabRDD_ex = sc.parallelize([(0.5, 1), (0.99, 0), (0.01, 1)])\n",
    "logLoss_ex = logLoss(probLabRDD_ex)\n",
    "print logLoss_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.allclose(logLoss_ex, 3.30116251751), 'Check your logLoss function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now define a sigmoid function that will be transforming linear combinations of feature values and weights to probabilities\n",
    "\n",
    "$$ f(z) = \\frac{1}{1+exp(-z)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "1.0\n",
      "3.72007597602e-44\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "# define sigmoid function\n",
    "# input: z, a float, result of a dot product between weights and feature values\n",
    "# output: a float, dot product transformed to 0-1 range\n",
    "def sigmoid(z):\n",
    "    return 1/(1+exp(-z))\n",
    "\n",
    "print sigmoid(0)  # should produce 0.5\n",
    "print sigmoid(100)  # should produce number close to 1\n",
    "print sigmoid(-100)  # should produce number close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will break down the gradient descent algorithm into components. We will write a function for the part that is computed on separate nodes in parallel - gradient. Then these gradients are summed in the driver node and weights are updated based on the whole sum.\n",
    "\n",
    "Define a function that computes the gradient for a single observation \n",
    "\n",
    "$$ (y-f(\\mathbf{w}^\\top \\mathbf{x})) \\mathbf{x}$$. \n",
    "\n",
    "Pay particular care that your gradient computation is correct. Error in gradients is the most frequent reason why something goes wrong with gradient descent.\n",
    "\n",
    "You will use the `DenseVector` [dot](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector.dot) method to compute a cross product between feature vector and weight vector. I provided you with an example on which you can test it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.05902226994e-07,3.05902226994e-07,3.05902226994e-07,3.05902226994e-07,3.05902226994e-07]\n"
     ]
    }
   ],
   "source": [
    "# compute the gradient for a single observation\n",
    "# inputs: weights, an array of regression coefficients; \n",
    "#         lp, a LabeledPoint of a single observation\n",
    "# output: DenseVector, an array of values, same length as weights\n",
    "def gradient(weights, lp):\n",
    "    return (lp.label - sigmoid(weights.dot(lp.features)))*lp.features\n",
    "    \n",
    "    \n",
    "\n",
    "weights_ex = DenseVector([1, 2, 3, 4, 5])\n",
    "lp_ex = LabeledPoint(1.0, [1, 1, 1, 1, 1])\n",
    "gradient_ex = gradient(weights_ex, lp_ex)\n",
    "print gradient_ex\n",
    "# you should see a following vector: \n",
    "# [3.05902226994e-07,3.05902226994e-07,3.05902226994e-07,3.05902226994e-07,3.05902226994e-07]\n",
    "# sigmoid should produce smth close to 1 so when it is subtracted from 1 we get smth close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now you should define a function that creates (probability, label) tuples that we will feed into logLoss() function to keep track of the loss function while we are training the algorithm via gradient descent. Keeping track of what is going on with the loss is one of the diagnostic tools you should always make use of to verify that gradient descent is converging.\n",
    "\n",
    "Use again the `dot` method to compute the prediction based on vector of weights and feature part of the observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9525741268224334, 1), (0.11920292202211755, 0)]\n",
      "0.0877576813084\n"
     ]
    }
   ],
   "source": [
    "# function should operate on a single observation\n",
    "# input: weights, a numpy array, and lp, a LabeledPoint\n",
    "# output: a tuple consisting of a predicted probability and a label\n",
    "def probLabTuple(weights, lp):\n",
    "    prob = sigmoid(weights.dot(lp.features))\n",
    "    return (prob, classify(prob, threshold))\n",
    "\n",
    "# lets check it on an easy example\n",
    "weights_ex = np.array([1, 2, 3, 4, 5])\n",
    "data_ex = sc.parallelize([LabeledPoint(1, np.array([-1, -1, -1, 1, 1])),\n",
    "                          LabeledPoint(0, np.array([-2, 2, -2, -2, 2]))])\n",
    "probLab_ex = data_ex.map(lambda lp: probLabTuple(weights_ex, lp))\n",
    "print probLab_ex.collect()\n",
    "# you should see a list of tuples: [(0.9525741268224334, 1.0), (0.11920292202211755, 0.0)]\n",
    "\n",
    "# what log loss this produces? you should see the following result: 0.0877576813084\n",
    "print logLoss(probLab_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now you are ready to define a gradient descent function. There is also a small sample on which you can test it out. \n",
    "\n",
    "Use the subset of the data also to determine the best learning rate `alpha`. Try out several values, too big learning rate will lead to increase of loss, for big ones it will appear flat or very erratic, while for appropriate rate the loss will generally decrease and flatten out after enough iterations. Too small rates will work fine but will require more iterations. \n",
    "\n",
    "How many iterations should you leave it run? That's difficult to say, it depends on application, you could monitor cost and after it flattened out you can stop. Better indicator is to actually compute the validation error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.481481481481,0.0530703538024,0.957379279075,0.859735349716,0.0,0.9375,0.698752922837,0.602510460251]), LabeledPoint(0.0, [0.493827160494,0.0302968686458,0.957379279075,0.859735349716,0.0,0.9375,0.698752922837,0.602510460251])]\n",
      "weights\n",
      "[-0.42352436  0.27495643 -1.02088121 -0.91676067  0.         -0.99968336\n",
      " -0.74510045 -0.64247433]\n",
      "Log loss\n",
      "[ 0.69314718  0.17284423  0.11109423  0.0845788   0.06962945  0.06000285\n",
      "  0.05328611  0.04834     0.04455378  0.04156959  0.03916349  0.03718796\n",
      "  0.03554176  0.03415307  0.03296951  0.03195192  0.03107046  0.03030196\n",
      "  0.02962816  0.0290345   0.02850916  0.02804251  0.02762658  0.02725474\n",
      "  0.02692142  0.0266219   0.02635218  0.02610881  0.02588884  0.0256897\n",
      "  0.02550916  0.02534527  0.02519631  0.02506079  0.02493736  0.02482486\n",
      "  0.02472222  0.02462853  0.02454293  0.02446468  0.02439312  0.02432763\n",
      "  0.02426767  0.02421276  0.02416244  0.02411632  0.02407403  0.02403525\n",
      "  0.02399966  0.02396701  0.02393704  0.02390953  0.02388426  0.02386106\n",
      "  0.02383975  0.02382017  0.02380218  0.02378565  0.02377046  0.0237565\n",
      "  0.02374366  0.02373187  0.02372102  0.02371105  0.02370189  0.02369346\n",
      "  0.02368571  0.02367858  0.02367203  0.023666    0.02366046  0.02365537\n",
      "  0.02365068  0.02364637  0.02364241  0.02363877  0.02363542  0.02363234\n",
      "  0.0236295   0.0236269   0.02362451  0.0236223   0.02362028  0.02361842\n",
      "  0.02361671  0.02361514  0.0236137   0.02361237  0.02361115  0.02361003\n",
      "  0.02360901  0.02360806  0.0236072   0.0236064   0.02360567  0.023605\n",
      "  0.02360439  0.02360382  0.02360331  0.02360284]\n"
     ]
    }
   ],
   "source": [
    "# function that performs the gradient descent on a given RDD \n",
    "# inputs: dataRDD, rdd with LabeledPoint observations, \n",
    "#         noIter, integer defining number of iterations\n",
    "#         alpha, float, learning rate\n",
    "#         diagnostics, bool, should the error be recorded?\n",
    "# output: a tuple consisting of two numpy arrays, one will be final weights, \n",
    "#         the other loss in each iteration\n",
    "def gradientDescent(dataRDD, noIter, alpha, diagnostics):\n",
    "    \n",
    "    # some help variables and arrays for storing the results\n",
    "    noObs = dataRDD.count()\n",
    "    noFeatures = len(dataRDD.take(1)[0].features)\n",
    "    error = np.zeros(noIter)  # we will keep track of misclassification error with each iteration\n",
    "    weights = np.zeros(noFeatures)  # initialize weights to zero's\n",
    "    \n",
    "    # we iterate until convergence, or in our case, defined by fixed number of iterations\n",
    "    for i in range(noIter):\n",
    "\n",
    "        # we will also keep track of the loss, compute the loss here with the current weights\n",
    "        # use probLabTuple() function defined above and apply it on whole dataRDD\n",
    "        if diagnostics:\n",
    "            probLab = dataRDD.map(lambda x: probLabTuple(weights, x))\n",
    "            error[i] = logLoss(probLab)\n",
    "        \n",
    "        # compute mean gradient (computed in the driver node)\n",
    "        # use the gradient() function you defined earlier\n",
    "        # meanGradient should be a DenseVector and have same number of features as weights vector\n",
    "        meanGradient = dataRDD.map(lambda x: gradient(weights, x)).mean()\n",
    "        weights += meanGradient  # we update the weights based on the totalGradient\n",
    "    \n",
    "    if diagnostics:\n",
    "        return weights, error\n",
    "    else:\n",
    "        return weights\n",
    "\n",
    "\n",
    "# ----\n",
    "# Check it on a smaller subset of data\n",
    "# ----\n",
    "\n",
    "# subset of the data\n",
    "data_ex = (sc\n",
    "            .parallelize(trainRDD.take(300))\n",
    "            .map(lambda lp: LabeledPoint(lp.label, lp.features)))\n",
    "print data_ex.take(2)\n",
    "\n",
    "# set the parameters, you can use a smaller subset to verify the learning rate\n",
    "# try other alpha values 76, 36, 26, 6, 0.6\n",
    "alpha = 6\n",
    "noIter = 100\n",
    "diagnostics = True\n",
    "\n",
    "# run the GD\n",
    "weights_ex, error_ex = gradientDescent(data_ex, noIter, alpha, diagnostics)\n",
    "print \"weights\\n\", weights_ex\n",
    "print \"Log loss\\n\", error_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now lets check how the loss is behaving in each iteration. If loss is not decreasing over time, you should try other learning rates, and if you don't get any improvements, something might be wrong with your gradient function.\n",
    "\n",
    "Note that we computed the loss over whole dataset. This works on a smaller subset when you are trying to set the learning rate, with the whole dataset (if it is a big one) this might take too much time and you should turn off that computation. If you use stochastic gradient descent instead, you can compute loss of the same observation or batch of observations that are used in the stochastic gradient descent (this is a small extension of the current gradientDescent function where the weight vector is updated after processing each observation or a small batch of randomly chosen observations). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/IPythonNB/venv/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6M3Mcs6B3sws5/4/OPDTVOSuDtEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf0c6d99d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# do some diagnostics\n",
    "x = range(0, len(error_ex))\n",
    "plt.plot(x, error_ex)\n",
    "plt.title('Log loss change with iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Apply gradient descent on whole dataset\n",
    "\n",
    "Now let's train the logistic regression model on all of our training data and evaluate its accuracy on the validation set. You would most likely need many more iterations than 500 to get better estimates.\n",
    "\n",
    "With the dataset of this size gradient descent works fine and you can easily run 10000 iterations on this dataset. However, on big datasets you would use stochastic version. We will not implement it, instead, now that you understand much better map-reduce operations that are going on under the hood, go to the next section and use MLlib function for training the logistic regression. That function uses stochastic gradient descent and provides you with several other useful options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the parameters, setting diagnostics to True will increase the time \n",
    "# with 14 workers this took about 5 min\n",
    "alpha = 6\n",
    "noIter = 500\n",
    "diagnostics = False\n",
    "\n",
    "\n",
    "# run the gradient descent on whole training set\n",
    "weights_gd = gradientDescent(normedRDD, noIter, alpha, diagnostics)\n",
    "print \"weights\\n\", weights_gd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# threshold parameter for misclassification\n",
    "threshold = 0.5\n",
    "\n",
    "# compute the log loss and missclassification error on training \n",
    "probLabTrain = trainRDD.map(lambda x: probLabTuple(weights_gd, x))\n",
    "logLossTrain_gd = logLoss(probLabTrain)\n",
    "misErrorTrain_gd = misError(probLabTrain, threshold)\n",
    "\n",
    "print logLossTrain_gd, misErrorTrain_gd\n",
    "\n",
    "# and validation set\n",
    "probLabVal = valRDD.map(lambda x: probLabTuple(weights_gd, x))\n",
    "logLossVal_gd = logLoss(probLabVal)\n",
    "misErrorVal_gd = misError(probLabVal, threshold)\n",
    "\n",
    "print logLossVal_gd, misErrorVal_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare it with the logLoss of the baseline model\n",
    "# training set\n",
    "probLabTrain = trainRDD.map(lambda x: probLabTuple(weights_gd, x))\n",
    "logLossTrain_base = logLoss(probLabTrain)\n",
    "\n",
    "print logLossTrain_base, baseTrain_misError\n",
    "\n",
    "# validation set\n",
    "probLabVal = valRDD.map(lambda x: probLabTuple(weights_gd, x))\n",
    "logLossVal_base = logLoss(probLabVal)\n",
    "\n",
    "print logLossVal_base, baseVal_misError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Logistic regression with MLlib\n",
    "\n",
    "We will now use [LogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithSGD) function from the MLlib Spark library. It returns a [LogisticRegressionModel](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LogisticRegressionModel) with plenty of useful methods.  You can access the weight by using the `LogisticRegressionModel.weights` attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "# set parameters\n",
    "noIter = 500\n",
    "alpha = 60  # MLlib algorithm uses decaying learning rate, so we need to increase it\n",
    "batchSize = 0.003  # defined as proportion of the whole dataset, ~100 observations\n",
    "regLambda = 1e-6\n",
    "regType = 'l2'\n",
    "intercept = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training the model on trainRDD\n",
    "model_sgd = (LogisticRegressionWithSGD.train(trainRDD, iterations = noIter,\n",
    "                                             step = alpha,\n",
    "                                             miniBatchFraction = batchSize,\n",
    "                                             regParam = regLambda,\n",
    "                                             regType = regType,\n",
    "                                             intercept = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the weights\n",
    "weights_sgd = model_sgd.weights\n",
    "print weights_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# create probLabRDD and compute the log loss and missclassification error on training, using the new SGD weights\n",
    "probLabTrain = trainRDD.map(lambda lp: probLabTuple(weights_sgd, lp))\n",
    "logLossTrain_sgd = logLoss(probLabTrain)\n",
    "misErrorTrain_sgd = misError(probLabTrain, threshold)\n",
    "\n",
    "print logLossTrain_sgd, misErrorTrain_sgd\n",
    "\n",
    "# for validation set, using the new SGD weights\n",
    "probLabVal = valRDD.map(lambda lp: probLabTuple(weights_sgd, lp))\n",
    "logLossVal_sgd = logLoss(probLabVal)\n",
    "misErrorVal_sgd = misError(probLabVal, threshold)\n",
    "\n",
    "print logLossVal_sgd, misErrorVal_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Optimizing the hyperparameters\n",
    "\n",
    "Do a grid search to find a good regularization parameter lambda and good learning rate.  \n",
    "Try `regLambda` values [1e-6, 1e-3, 1] and alpha values [0.1, 1, 10, 50, 100].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noIter = 500\n",
    "batchSize = 0.003  # defined as proportion of the whole dataset, ~100 observations\n",
    "regType = 'l2'\n",
    "intercept = False\n",
    "modelLoss = []  # storing losses of each model\n",
    "\n",
    "for alpha in [0.1, 1, 10, 50, 100]:\n",
    "    for regLambda in [1e-6, 1e-3, 1]:\n",
    "        model = (LogisticRegressionWithSGD.train(trainRDD, iterations = noIter,\n",
    "                                             step = alpha,\n",
    "                                             miniBatchFraction = batchSize,\n",
    "                                             regParam = regLambda,\n",
    "                                             regType = regType,\n",
    "                                             intercept = False))\n",
    "        \n",
    "        # evaluate the model\n",
    "        probLabVal = valRDD.map(lambda lp: probLabTuple(weights_sgd, lp))\n",
    "        logLossVal = logLoss(probLabVal)\n",
    "        modelLoss.append(logLossVal)\n",
    "        \n",
    "        # some printout in each iteration\n",
    "        print 'alpha = {0:.0e}, lambda = {1}, loss = {2:.3f}'.format(alpha, regLambda, logLossVal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda of 1 and alpha of 50 seems to be a good solution. Lets use it for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "noIter = 500\n",
    "alpha = 50  \n",
    "batchSize = 0.003  \n",
    "regLambda = 1\n",
    "regType = 'l2'\n",
    "intercept = False\n",
    "\n",
    "# train it on trainRDD\n",
    "theModel = (LogisticRegressionWithSGD.train(trainRDD, iterations = noIter,\n",
    "                                             step = alpha,\n",
    "                                             miniBatchFraction = batchSize,\n",
    "                                             regParam = regLambda,\n",
    "                                             regType = regType,\n",
    "                                             intercept = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the final model\n",
    "\n",
    "# create probLabRDD and compute the log loss and missclassification error on training, using the new SGD weights\n",
    "probLabTrain = trainRDD.map(lambda lp: probLabTuple(theModel.weights, lp))\n",
    "logLossTrain_sgd = logLoss(probLabTrain)\n",
    "misErrorTrain_sgd = misError(probLabTrain, threshold)\n",
    "\n",
    "print logLossTrain_sgd, misErrorTrain_sgd\n",
    "\n",
    "# for validation set, using the new SGD weights\n",
    "probLabVal = valRDD.map(lambda lp: probLabTuple(theModel.weights, lp))\n",
    "logLossVal_sgd = logLoss(probLabVal)\n",
    "misErrorVal_sgd = misError(probLabVal, threshold)\n",
    "\n",
    "print logLossVal_sgd, misErrorVal_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finally, assesing our final chosen model on the test set\n",
    "probLabTest = testRDD.map(lambda lp: probLabTuple(theModel.weights, lp))\n",
    "logLossTest_sgd = logLoss(probLabTest)\n",
    "misErrorTest_sgd = misError(probLabTest, threshold)\n",
    "\n",
    "print logLossTest_sgd, misErrorTest_sgd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
